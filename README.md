# Learning-Loop-ML-Edition
Welcome to "Learning Loop : ML Edition"!

In this repository, Each day, I'll be sharing my discoveries, insights, and lessons learned as I delve deeper into the fascinating realm of AI.

Happy Learning!

## Day 1 - 7/20/2023
- I got introduced to Multihead Attention, its mechanism in general. In short it is for loop over self attention mechanism ( which I learned yesterday ). Furthermore, I got intuition on Transformer Network in detail. The transformer network works by processing input data through layers of self-attention mechanisms and feed-forward neural networks. It utilizes self-attention to capture contextual information and dependencies within the input sequence, enabling efficient parallel processing. 
