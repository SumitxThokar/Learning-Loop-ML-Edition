# Learning-Loop-ML-Edition
Welcome to "Learning Loop : ML Edition"!

In this repository, Each day, I'll be sharing my discoveries, insights, and lessons learned as I delve deeper into the fascinating realm of AI.

Happy Learning!

## Day 1 - 7/20/2023
- I got introduced to Multihead Attention, its mechanism in general. In short it is for loop over self attention mechanism ( which I learned yesterday ). Furthermore, I got intuition on Transformer Network in detail. The transformer network works by processing input data through layers of self-attention mechanisms and feed-forward neural networks. It utilizes self-attention to capture contextual information and dependencies within the input sequence, enabling efficient parallel processing. 

## Day 2 - 7/21/2023
- Today was the day of assignment. I worked on Transformer Architecture assignment where in the assignment I implemented the components of the transformers model in TensorFlow 2.4. The assignment was the great source to learn more indepth on Transformer where I implemented Position Encoding, Masking both padding and Look-ahead, Self- Attention, Encoder, Decoder and lastly prepared the model. 

## Day 3 - 7/22/2023
-  I deepened my knowledge on Transformer. Today I delved into the pre-processing methods applied to raw text before passing it to the encoder and decoder blocks of the transformer architecture. I learned that difference between two vectors seperated by k position is always constant and Positional Encoding (PE) can affect Word Embeddings if relative weights of PE is not small. 

## Day 4 - 7/23/2023
- Today I optimized Transformer model to perform Name Entity Recognition on dataset of resumes where I used tokenizers and pretrained model form Hugging Face Library and Fine-tuned a pre-trained transformer model for Named-Entity Recognition.

## Day 5 - 7/24/2023
- Today I was introducted to Generative Adversarial Networks (GANs). GANs are used to generate incredibly realistic images/ music. I learned what Generative models are and learned the difference of Discriminative and generative Model. Furthermore I got intuitions on types of Generative models and how they work: VAEs and GANs. Lastly I learned Real life GANs applications and big techs that are planning or using this cool technique to enhance.

## Day 6 - 7/25/2023
- Today I learned the intuitions behind GANs and learned about the goals of Generator, discriminator and competition between them. Furthermore I learned how discriminator classifier distinguish between classes of Real and Fake labels.

## Day 7 - 7/26/2023
-  Today I learned briefly about how Generator generates examples of the class with noise vector and input. The I learned about how Binary Cross Entropy funciton is used for training GANs and lastly Putting It All Together learned about the architecture of GANs as whole. 

## Day 8 - 7/27/2023
- I was introducted to functionality of Pytorch and cleared few question like why pytorch. Next, I implemeted tensor's operations like addition, multiplication using pytorch. Furthermore I implemeted simple Neural Network using pytorch's nn library. This was all done in guided assignment which introduced pytorch. 
